{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Machine Learning Concepts\n",
    "\n",
    "In this notebook we're going to explore some simple concepts for machine learning, and then apply them to understand how the basic Regression and Classification algorithms work: Linear Regression, and Logisitic Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "Neural networks are a class of machine learning algorithms used to model complex patterns in datasets using multiple hidden layers and non-linear activation functions. \n",
    "\n",
    "A neural network takes an input, passes it through multiple layers of hidden neurons (mini-functions with unique coefficients that must be learned), and outputs a prediction representing the combined input of all the neurons.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neurons\n",
    "\n",
    "A neuron takes a group of weighted inputs, applies an activation function, and returns an output.\n",
    "\n",
    "\n",
    "**Inputs**\n",
    "- Inputs to a neuron can either be features from a training set or outputs from a previous layer’s neurons. \n",
    "\n",
    "**Weights**\n",
    "- Weights are applied to the inputs as they travel along synapses to reach the neuron. \n",
    "- A neuron’s input equals the sum of weighted outputs from all neurons in the previous layer. \n",
    "- Each input is multiplied by the weight associated with the synapse connecting the input to the current neuron. \n",
    "    - e.g. If there are 3 inputs or neurons in the previous layer, each neuron in the current layer will have 3 distinct weights — one for each each synapse.\n",
    "\n",
    "- Single Input\n",
    "\n",
    "    - 𝑍=𝐼𝑛𝑝𝑢𝑡⋅𝑊𝑒𝑖𝑔ℎ𝑡=𝑋𝑊\n",
    "- Multiple Inputs\n",
    "\n",
    "    - 𝑍=∑𝑖=1𝑛𝑥𝑖𝑤𝑖 \\\n",
    "    =𝑥1𝑤1+𝑥2𝑤2+𝑥3𝑤3\n",
    "\n",
    "**Activation Function**\n",
    "- Activation functions give neural networks their power — allowing them to model complex non-linear relationships.\n",
    "- By modifying inputs with non-linear functions neural networks can model highly complex relationships between features. \n",
    "\n",
    "** Activation Function Properties**\n",
    " - Non-linear - In linear regression we’re limited to a prediction equation that looks like a straight line. This is nice for simple datasets with a one-to-one relationship between inputs and outputs, but what if the patterns in our dataset were non-linear? (e.g. 𝑥2, sin, log). To model these relationships we need a non-linear prediction equation.¹ Activation functions provide this non-linearity.\n",
    "-  Continuously differentiable — To improve our model with gradient descent, we need our output to have a nice slope so we can compute error derivatives with respect to weights. If our neuron instead outputted 0 or 1 (perceptron), we wouldn’t know in which direction to update our weights to reduce our error.\n",
    " - Fixed Range — Activation functions typically squash the input data into a narrow range that makes training the model more stable and efficient.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "- The neuron applies an activation function to the “sum of weighted inputs” from each incoming synapse and passes the result on to all the neurons in the next layer.\n",
    "\n",
    "#### Activation Function Properties\n",
    "\n",
    " - Non-linear - In linear regression we’re limited to a prediction equation that looks like a straight line. This is nice for simple datasets with a one-to-one relationship between inputs and outputs, but what if the patterns in our dataset were non-linear? (e.g. 𝑥2, sin, log). To model these relationships we need a non-linear prediction equation.¹ Activation functions provide this non-linearity.\n",
    "-  Continuously differentiable — To improve our model with gradient descent, we need our output to have a nice slope so we can compute error derivatives with respect to weights. If our neuron instead outputted 0 or 1 (perceptron), we wouldn’t know in which direction to update our weights to reduce our error.\n",
    " - Fixed Range — Activation functions typically squash the input data into a narrow range that makes training the model more stable and efficient.\n",
    " \n",
    " \n",
    "#### Types of Activation Functions\n",
    " - **Linear**: A straight line function where activation is proportional to input ( which is the weighted sum from neuron ).\n",
    "    - Pros:\n",
    "\n",
    "        - It gives a range of activations, so it is not binary activation.\n",
    "        - We can definitely connect a few neurons together and if more than 1 fires, we could take the max ( or softmax) and decide based on that.\n",
    "    - Cons:\n",
    "\n",
    "        - For this function, derivative is a constant. That means, the gradient has no relationship with X.\n",
    "        - It is a constant gradient and the descent is going to be on constant gradient.\n",
    "        - If there is an error in prediction, the changes made by back propagation is constant and not depending on the change in input delta(x)!\n",
    "        \n",
    " - **Exponential Linear Unit (ELU)**:  a function that tend to converge cost to zero faster and produce more accurate results. Different to other activation functions, ELU has a extra alpha constant which should be positive number. ELU is very similiar to RELU except negative inputs. They are both in identity function form for non-negative inputs. On the other hand, ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "    - Pros:\n",
    "\n",
    "        - ELU becomes smooth slowly until its output equal to -α whereas RELU sharply smoothes.\n",
    "        - ELU is a strong alternative to ReLU.\n",
    "        - Unlike to ReLU, ELU can produce negative outputs.\n",
    "    - Cons:\n",
    "        - For x > 0, it can blow up the activation with the output range of [0, inf].\n",
    " - **Rectified Linear Units (ReLu)**: The formula is deceptively simple: 𝑚𝑎𝑥(0,𝑧). Despite its name and appearance, it’s not linear and provides the same benefits as Sigmoid but with better performance.\n",
    " \n",
    "    - Pros:\n",
    "        - It avoids and rectifies vanishing gradient problem.\n",
    "        - ReLu is less computationally expensive than tanh and sigmoid because it involves simpler mathematical operations.\n",
    "    - Cons:\n",
    "        - One of its limitation is that it should only be used within Hidden layers of a Neural Network Model.\n",
    "        - Some gradients can be fragile during training and can die. It can cause a weight update which will makes it never activate on any data point again. Simply saying that ReLu could result in Dead Neurons.\n",
    "        - In another words, For activations in the region (x<0) of ReLu, gradient will be 0 because of which the weights will not get adjusted during descent. That means, those neurons which go into that state will stop responding to variations in error/ input ( simply because gradient is 0, nothing changes ). This is called dying ReLu problem.\n",
    "        - The range of ReLu is [0, inf). This means it can blow up the activation.\n",
    " \n",
    "- **Sigmoid**: Sigmoid takes a real value as input and outputs another value between 0 and 1. It’s easy to work with and has all the nice properties of activation functions: it’s non-linear, continuously differentiable, monotonic, and has a fixed output range.\n",
    "\n",
    "     - Pros:\n",
    "         - It is nonlinear in nature. Combinations of this function are also nonlinear!\n",
    "         - It will give an analog activation unlike step function.\n",
    "         - It has a smooth gradient too.\n",
    "         - It’s good for a classifier.\n",
    "         - The output of the activation function is always going to be in range (0,1) compared to (-inf, inf) of linear function. So we have our activations bound in a range. Nice, it won’t blow up the activations then.\n",
    "    - Cons:\n",
    "        - Towards either end of the sigmoid function, the Y values tend to respond very less to changes in X.\n",
    "        - It gives rise to a problem of “vanishing gradients”.\n",
    "        - Its output isn’t zero centered. It makes the gradient updates go too far in different directions. 0 < output < 1, and it makes optimization harder.\n",
    "        - Sigmoids saturate and kill gradients.\n",
    "        - The network refuses to learn further or is drastically slow ( depending on use case and until gradient /computation gets hit by floating point value limits ).\n",
    "        \n",
    "- **Tanh**: Tanh squashes a real-valued number to the range [-1, 1]. It’s non-linear. But unlike Sigmoid, its output is zero-centered. Therefore, in practice the tanh non-linearity is always preferred to the sigmoid nonlinearity. [1]\n",
    "\n",
    "    - Pros:\n",
    "        - The gradient is stronger for tanh than sigmoid ( derivatives are steeper).\n",
    "    - Cons:\n",
    "        - Tanh also has the vanishing gradient problem.\n",
    "\n",
    "- **SoftMax**: Softmax function calculates the probabilities distribution of the event over ‘n’ different events. In general way of saying, this function will calculate the probabilities of each target class over all possible target classes. Later the calculated probabilities will be helpful for determining the target class for the given inputs.\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward Propagation\n",
    "\n",
    "*Forward propagation is how neural networks make predictions.*\n",
    "\n",
    "Input data is “forward propagated” through the network layer by layer to the final layer which outputs a prediction. For the toy neural network, a single pass of forward propagation translates mathematically to:\n",
    "\n",
    "𝑃𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛=𝐴(𝐴(𝑋𝑊ℎ)𝑊𝑜)\n",
    "\n",
    "\n",
    "Where 𝐴 is an activation function like ReLU, 𝑋 is the input and 𝑊ℎ and 𝑊𝑜 are weights.\n",
    "\n",
    "**Steps**\n",
    "1. Calculate the weighted input to the hidden layer by multiplying 𝑋 by the hidden weight 𝑊ℎ\n",
    "2. Apply the activation function and pass the result to the final layer\n",
    "3. Repeat step 2 except this time 𝑋 is replaced by the hidden layer’s output, 𝐻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Forward Propagation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def init_weights():\n",
    "    Wh = np.random.randn(INPUT_LAYER_SIZE, HIDDEN_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/INPUT_LAYER_SIZE)\n",
    "    Wo = np.random.randn(HIDDEN_LAYER_SIZE, OUTPUT_LAYER_SIZE) * \\\n",
    "                np.sqrt(2.0/HIDDEN_LAYER_SIZE)\n",
    "\n",
    "    return Wh, Wo\n",
    "\n",
    "\n",
    "    \n",
    "def init_bias():\n",
    "    Bh = np.full((1, HIDDEN_LAYER_SIZE), 0.1)\n",
    "    Bo = np.full((1, OUTPUT_LAYER_SIZE), 0.1)\n",
    "    return Bh, Bo\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_prime(Z):\n",
    "    '''\n",
    "    Z - weighted input matrix\n",
    "\n",
    "    Returns gradient of Z where all\n",
    "    negative values are set to 0 and\n",
    "    all positive values set to 1\n",
    "    '''\n",
    "    Z[Z < 0] = 0\n",
    "    Z[Z > 0] = 1\n",
    "    return Z\n",
    "\n",
    "def cost(yHat, y):\n",
    "    cost = np.sum((yHat - y)**2) / 2.0\n",
    "    return cost\n",
    "\n",
    "def cost_prime(yHat, y):\n",
    "    return yHat - y\n",
    "\n",
    "def feed_forward(X):\n",
    "    '''\n",
    "    X    - input matrix\n",
    "    Zh   - hidden layer weighted input\n",
    "    Zo   - output layer weighted input\n",
    "    H    - hidden layer activation\n",
    "    y    - output layer\n",
    "    yHat - output layer predictions\n",
    "    '''\n",
    "\n",
    "    # Hidden layer\n",
    "    Zh = np.dot(X, Wh) + Bh\n",
    "    H = relu(Zh)\n",
    "\n",
    "    # Output layer\n",
    "    Zo = np.dot(H, Wo) + Bo\n",
    "    yHat = relu(Zo)\n",
    "    return yHat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Data Shape (3, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1.95089128, 0.60569669],\n",
       "       [1.95089128, 0.60569669],\n",
       "       [1.95089128, 0.60569669]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[2,2,2]]).T\n",
    "print('Input Data Shape {}'.format(X.shape))\n",
    "INPUT_LAYER_SIZE = X.shape[1]\n",
    "HIDDEN_LAYER_SIZE = 2\n",
    "OUTPUT_LAYER_SIZE = 2\n",
    "Wh, Wo = init_weights()\n",
    "Bh, Bo = init_bias()\n",
    "feed_forward(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
